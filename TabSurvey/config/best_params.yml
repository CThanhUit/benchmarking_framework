
parameters: {
    ############################# CICDDoS2019 #######################################
    CICDDoS2019: {
      LinearModel: {},
      KNN: {
          n_neighbors: 23
      },
      XGBoost: {
        max_depth: 6, 
        alpha: 0.0007238741516358357,
        lambda: 0.5523548332144541,
        eta: 0.015027958840041676, 
        verbosity: 1, 
        tree_method": gpu_hist, 
        gpu_id: 0, 
        objective: binary:logistic, 
        eval_metric: auc
      },

      LightGBM: {
        num_leaves: 8,
        lambda_l1: 1.1000151409170677e-07,
        lambda_l2: 1.7989804352633825e-05,
        learning_rate: 0.12906704777849096,
        verbosity: -1, 
        objective': binary,
        metric: auc
      },


      TabNet: {
          n_d: 14,
          n_steps: 3,
          gamma: 1.76,
          cat_emb_dim: 2,
          n_independent: 3,
          n_shared: 2,
          momentum: 0.319,
          mask_type: entmax
      },
      TabTransformer: {
          dim: 128,
          depth: 1,
          heads: 4,
          weight_decay: -5,
          learning_rate: -5,
          dropout: 0.4
      },
      SAINT: {
          dim: 32,
          depth: 2,
          head: 8, 
          dropout: 0.5
        },
    },
        ############################# CICIoT2023 #######################################
    CICIoT2023: {
      KNN: {
          n_neighbors: 25
      },
      XGBoost: {
          max_depth: 5,
          lambda: 0.354,
          alpha: 0.132,
          eta: 0.054
      },
      LightGBM: {
          num_leaves: 24,
          lambda_l2: 1.5e-4,
          lambda_l1: 6.1e-5,
          learning_rate: 2.4e-2
      },
      TabNet: {
        n_d: 47,
        n_steps: 5, 
        gamma: 1.8832476712814077, 
        cat_emb_dim: 2, 
        n_independent: 1, 
        n_shared: 2, 
        momentum: 0.14479109369235799, 
        mask_type: entmax
        },
      TabTransformer: {
          dim: 128,
          depth: 1,
          heads: 4,
          weight_decay: -5,
          learning_rate: -5,
          dropout: 0.4
      },
      SAINT: {
          dim: 32,
          depth: 1,
          heads: 2,
          dropout: 0.6
      },
    },    ############################# CICDMalMem2022 #######################################
    CICMalMem2022: {
      LinearModel: {},
      KNN: {
          n_neighbors: 25
      },
      DecisionTree: {
          max_depth: 8
      },
      RandomForest: {
          max_depth: 12,
          n_estimators: 85
      },
      XGBoost: {
          max_depth: 5,
          lambda: 0.354,
          alpha: 0.132,
          eta: 0.054
      },
      CatBoost: {
          learning_rate: 0.05,
          max_depth: 8,
          l2_leaf_reg: 0.576
      },
      LightGBM: {
          num_leaves: 24,
          lambda_l2: 1.5e-4,
          lambda_l1: 6.1e-5,
          learning_rate: 2.4e-2
      },
      ModelTree: {
          criterion: gradient-renorm-z,
          max_depth: 3
      },
      MLP: {
          hidden_dim: 47,
          n_layers: 4,
          learning_rate: 8.4e-4
      },
      TabNet: {
          n_d: 14,
          n_steps: 3,
          gamma: 1.76,
          cat_emb_dim: 2,
          n_independent: 3,
          n_shared: 2,
          momentum: 0.319,
          mask_type: entmax
      },
      VIME: {
          p_m: 0.113,
          K: 15,
          alpha: 9.83,
          beta: 0.313
      },
      TabTransformer: {
          dim: 128,
          depth: 1,
          heads: 4,
          weight_decay: -5,
          learning_rate: -5,
          dropout: 0.4
      },
      NODE: {
          num_layers: 4,
          total_tree_count: 1024,
          tree_depth: 8,
          tree_output_dim: 3
      },
      DeepGBM: {
          n_trees: 200,
          maxleaf: 64,
          loss_de: 10,
          loss_dr: 0.7
      },
      RLN: {
          layers: 5,
          theta: -12,
          log_lr: 5,
          norm: 2
      },
      DNFNet: {
          n_forumlas: 1024,
          elastic_net_beta: 1.3
      },
      STG: {
          learning_rate: 4.64e-3,
          lam: 6.2e-3,
          hidden_dims: [500, 50, 10]
      },
      NAM: {
          lr: 1.2e-3,
          output_regularization: 2.95e-3,
          dropout: 1.8e-2,
          feature_dropout: 0.168,
      },
      DeepFM: {
          dnn_dropout: 0.4
      },
      SAINT: {
          dim: 32,
          depth: 1,
          heads: 2,
          dropout: 0.6
      },
      DANet: {
          layer: 8,
          base_outdim: 96,
          k: 4,
          drop_rate: 0
      },
    },    ############################# CICIDS2018 #######################################
    CICIDS2018: {
      KNN: {
          n_neighbors: 13
      },
      XGBoost: {
        max_depth: 7,
        alpha: 0.002061569467814489,
        lambda: 0.02525270878362098,
        eta: 0.11728583619999852,
        verbosity: 1,
        tree_method: gpu_hist,
        gpu_id: 0,
        objective: binary:logistic,
        eval_metric: auc
      },
      LightGBM: {
        num_leaves: 3606,
        lambda_l1: 0.0002693622275118998, 
        lambda_l2: 0.0005319031164283219, 
        learning_rate: 0.18761620645762767, 
        verbosity: -1, 
        objective: binary,
        metric: auc
      },

      TabNet: {
          n_d: 14,
          n_steps: 3,
          gamma: 1.76,
          cat_emb_dim: 2,
          n_independent: 3,
          n_shared: 2,
          momentum: 0.319,
          mask_type: entmax
      },
      TabTransformer: {
          dim: 128,
          depth: 1,
          heads: 4,
          weight_decay: -5,
          learning_rate: -5,
          dropout: 0.4
      },
      SAINT: {
          dim: 32,
          depth: 1,
          heads: 2,
          dropout: 0.6
      },
    },    ############################# CICIDS2017 #######################################
    CICIDS2017: {
      LinearModel: {},
      KNN: {
          n_neighbors: 25
      },
      DecisionTree: {
          max_depth: 8
      },
      RandomForest: {
          max_depth: 12,
          n_estimators: 85
      },
      XGBoost: {
          max_depth: 5,
          lambda: 0.354,
          alpha: 0.132,
          eta: 0.054
      },
      CatBoost: {
          learning_rate: 0.05,
          max_depth: 8,
          l2_leaf_reg: 0.576
      },
      LightGBM: {
          num_leaves: 24,
          lambda_l2: 1.5e-4,
          lambda_l1: 6.1e-5,
          learning_rate: 2.4e-2
      },
      ModelTree: {
          criterion: gradient-renorm-z,
          max_depth: 3
      },
      MLP: {
          hidden_dim: 47,
          n_layers: 4,
          learning_rate: 8.4e-4
      },
      TabNet: {
          n_d: 14,
          n_steps: 3,
          gamma: 1.76,
          cat_emb_dim: 2,
          n_independent: 3,
          n_shared: 2,
          momentum: 0.319,
          mask_type: entmax
      },
      VIME: {
          p_m: 0.113,
          K: 15,
          alpha: 9.83,
          beta: 0.313
      },
      TabTransformer: {
          dim: 128,
          depth: 1,
          heads: 4,
          weight_decay: -5,
          learning_rate: -5,
          dropout: 0.4
      },
      NODE: {
          num_layers: 4,
          total_tree_count: 1024,
          tree_depth: 8,
          tree_output_dim: 3
      },
      DeepGBM: {
          n_trees: 200,
          maxleaf: 64,
          loss_de: 10,
          loss_dr: 0.7
      },
      RLN: {
          layers: 5,
          theta: -12,
          log_lr: 5,
          norm: 2
      },
      DNFNet: {
          n_forumlas: 1024,
          elastic_net_beta: 1.3
      },
      STG: {
          learning_rate: 4.64e-3,
          lam: 6.2e-3,
          hidden_dims: [500, 50, 10]
      },
      NAM: {
          lr: 1.2e-3,
          output_regularization: 2.95e-3,
          dropout: 1.8e-2,
          feature_dropout: 0.168,
      },
      DeepFM: {
          dnn_dropout: 0.4
      },
      SAINT: {
          dim: 32,
          depth: 1,
          heads: 2,
          dropout: 0.6
      },
      DANet: {
          layer: 8,
          base_outdim: 96,
          k: 4,
          drop_rate: 0
      },
    },
}